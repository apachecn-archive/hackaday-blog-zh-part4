# 《当机器学习变得怪异时的伦理:死亡机器人》

> 原文:[https://hack aday . com/2022/06/05/the-ethics-of-when-machine-learning-gets-verged-dead bots/](https://hackaday.com/2022/06/05/the-ethics-of-when-machine-learning-gets-weird-deadbots/)

每个人都知道聊天机器人是什么，但是一个死机器人呢？一个*死机器人*是一个聊天机器人，它的训练数据——决定它如何交流和交流什么——是基于一个已故的人的数据。现在让我们考虑一个名叫约书亚·巴尔博的人的例子，他创造了一个聊天机器人来模拟与他已故的未婚妻的对话。此外，最终为该项目提供动力的 GPT-3 API 的提供商 OpenAI 对此也有问题，因为他们的条款明确禁止将他们的 API 用于(除其他外)“色情”目的。

博士后研究员萨拉·苏亚雷斯·冈萨洛(Sara Suárez-Gonzalo)观察到，这个故事的事实被掩盖得足够好，但没有人从任何其他角度看待它。我们当然都知道这个案件的不同元素中充满了什么样的对错，但是我们能准确地解释为什么开发一个死机器人是好是坏吗？

这正是[萨拉]打算做的。她的文章引人入胜且细致入微，为主题提供了具体的指导。伤害可能吗？同意怎么会出现在这种事情中？谁为糟糕的结果负责？如果你对这类问题感兴趣，花点时间看看她的文章。

[Sara]证明了在某些条件下，创造一个死机器人是合乎道德的。简而言之，关键点是被模仿的人和开发并与之互动的人应该已经同意，并尽可能详细地描述系统的范围、设计和预期用途。(这种说法很重要，因为机器学习通常变化很快。如果某一天系统或能力不再像最初想象的那样，该怎么办？)对任何潜在负面结果的责任应该由发展者和从中受益者共同承担。

[Sara]指出，这个案例是一个完美的例子，说明了为什么机器学习的伦理真的很重要，如果不关注这些事情，[我们可以预计尴尬的问题会继续出现](https://hackaday.com/2020/09/26/twitter-its-not-the-algorithms-fault-its-much-worse/)。
# 神经形态计算:它是什么？我们在哪里？

> 原文:[https://hack aday . com/2021/08/19/neuromorphic-computing-what-it-and-where-we-at/](https://hackaday.com/2021/08/19/neuromorphic-computing-what-is-it-and-where-are-we-at/)

在过去的一百年左右的时间里，作为人类的集体，我们一直在梦想、思考、写作、歌唱和制作关于一台机器的电影，这台机器能够以与我们相似的方式思考、推理和智能。从山姆·巴特勒 1872 年出版的《Erewhon》开始的故事，埃德加·爱伦·坡的《梅尔泽尔的棋手》，以及 1927 年的电影《大都会》，都表明了机器可以像人一样思考和推理的想法。不是以魔法或幻想的方式。他们借鉴了古希腊和埃及的自动机，并结合了亚里士多德、拉蒙·勒尔、霍布斯以及成千上万其他哲学家的观点。

他们对人类思维的概念使他们相信所有理性思维都可以用代数或逻辑来表达。后来，电路、计算机和摩尔定律的出现导致人们不断猜测人类水平的智能就在眼前。一些人将其誉为人类的救世主，而另一些人则将灾难描绘为第二个智能实体崛起来粉碎第一个智能实体(人类)。

计算机化人工智能的火焰之前已经明亮地燃烧过几次，比如在 20 世纪 50 年代、80 年代和 21 世纪 10 年代。不幸的是，之前的两次人工智能繁荣都伴随着一个“人工智能冬天”，由于未能达到预期，这个冬天已经过时了。这个冬天通常被归咎于缺乏计算机能力、对大脑的了解不足，或者炒作和过度投机。在我们当前的人工智能夏季中，大多数人工智能研究人员专注于使用稳定增长的计算机能力来增加他们神经网络的深度。尽管它们的名字，神经网络是由大脑中的神经元启发的，并且只分享表面水平的相似性。

一些研究人员认为，人类水平的一般智能可以通过简单地向这些由不断增加的数据宝库提供的简化卷积系统添加越来越多的层来实现。这一点得到了这些网络可以产生的不可思议的东西的支持，并且每年都有所改善。然而，尽管深度神经网络产生了奇迹，但它们仍然只擅长一件事。如果没有人类加入，一个玩 AI 的超人 Atari 无法制作音乐或思考天气模式。此外，输入数据的质量极大地影响了网络的质量，并且做出推断的能力是有限的，[在某些领域产生了令人失望的结果](https://hackaday.com/2021/08/02/github-copilot-and-the-unfulfilled-promises-of-an-artificial-intelligence-future/)。一些人认为，循环神经网络永远不会获得我们大脑所提供的那种普遍的智能和灵活性。

然而，一些研究人员正试图创造更像大脑的东西，你猜对了，更接近地模仿大脑。鉴于我们正处于计算机架构的黄金时代，现在似乎是创造新硬件的时候了。这种类型的硬件被称为神经形态硬件。

## 什么是神经形态计算？

神经形态是对任何试图模仿或模拟大脑的软件或硬件的一个花哨术语。虽然我们对大脑还有许多不了解的地方，但在过去的几年里，我们已经取得了一些惊人的进展。一个普遍接受的理论是[柱状假说](https://en.wikipedia.org/wiki/Cortical_column)，该假说认为新皮层(被广泛认为是做出决定和处理信息的地方)是由数百万个皮质柱状物或皮质模块形成的。大脑的其他部分，如海马体，具有与后脑其他部分不同的可识别结构。

[![](../Images/d004c72f1056be0149c89fe2b0178271.png)](https://hackaday.com/wp-content/uploads/2017/06/biological.jpg) 新大脑皮层在结构上与后脑相当不同。我们知道特定功能发生在一般区域，如视觉和听觉，但从新皮层的结构角度来看，实际的大脑物质看起来非常相似。从更抽象的角度来看，视觉部分几乎与听觉部分相同，而后脑部分是独特的，是基于功能构建的。这种见解导致弗农·芒特卡斯尔推测，有一种中央算法或结构驱动着新大脑皮层中的所有处理。皮质骨柱是一个独特的单位，因为它通常有 6 层，层与层之间的垂直连接比水平连接更多。这意味着单个单元可以被重复复制以形成人工新皮层，这对超大规模集成电路(VLSI)技术来说是一个好兆头。我们的制造工艺特别适合在很小的表面区域内制造一百万个复制品。

虽然递归神经网络(RNN)是完全连接的，但真正的大脑对什么连接什么很挑剔。视觉网络的常见模型是分层金字塔，底层提取特征，每个后续特征提取更抽象的特征。大多数经过分析的大脑回路显示出各种各样的层次结构，这些层次结构的连接在自身上循环。反馈和前馈连接连接到层次结构中的多个级别。这种“级别跳跃”是常态，而不是规则，这表明这种结构可能是我们大脑表现出的特性的关键。

这将我们引向下一个整合点:大多数神经元网络使用[泄漏的整合-发射模型](https://en.wikipedia.org/wiki/Biological_neuron_model#Integrate-and-fire)。在 RNN 中，每个节点在每个时间步长发出一个信号，而真正的神经元只有在达到其膜电位时才会触发(现实比这复杂一点)。具有这种特性的更加生物学精确的人工神经网络(ANN)被称为[脉冲神经网络(SNN)](https://en.wikipedia.org/wiki/Spiking_neural_network) 。这个漏洞百出的“整合-发射”模型在生物学上不如其他模型准确，如[欣德马什-罗斯模型](https://en.wikipedia.org/wiki/Hindmarsh%E2%80%93Rose_model)或[霍奇金-赫胥黎模型](https://en.wikipedia.org/wiki/Hodgkin%E2%80%93Huxley_model)。它们模拟神经递质化学物质和突触间隙。尽管如此，它的计算成本要高得多。鉴于神经元并不总是放电，这确实意味着数字需要用[尖峰脉冲串](https://en.wikipedia.org/wiki/Spike_train)来表示，数值编码为速率码、尖峰时间或频率码。

## 我们的进展如何？

一些小组已经在直接模拟神经元，例如 [OpenWorm 项目模拟了一种叫做秀丽隐杆线虫的蛔虫中的 302 个神经元。](https://hackaday.com/2014/12/15/gift-your-next-robot-with-the-brain-of-a-roundworm/)这些项目中许多项目的当前目标是继续增加神经元数量、模拟精度，以及[提高程序效率](https://www.frontiersin.org/articles/10.3389/fninf.2019.00063/full)。例如，在德国，一个名为 [SpiNNaker](http://apt.cs.manchester.ac.uk/projects/SpiNNaker/) 的项目是一台实时模拟 10 亿个神经元的低级超级计算机。该项目在 2018 年末达到了一百万个核心，在 2019 年，他们宣布了一大笔拨款，将资助二代机(SpiNNcloud)的建设。

许多公司、政府和大学正在寻找奇异的材料和技术来创造人工神经元，如[忆阻器](https://hackaday.com/2020/06/13/engineers-develop-a-brain-on-a-chip/)、[自旋扭矩振荡器](https://en.wikipedia.org/wiki/Spin-transfer_torque)和[磁性约瑟夫森结器件](https://aip.scitation.org/doi/pdf/10.1063/1.5042425) (MJJs)。虽然其中许多在模拟中看起来令人难以置信地有前途，但在模拟中(或在小型开发板上)的 12 个神经元与实现真正人类水平的能力所需的数千甚至数十亿个神经元之间存在巨大差距。

[![](../Images/e3fb5b31de8a92d36cd5bf7c24deb57d.png)](https://hackaday.com/wp-content/uploads/2021/08/Rich-Uhlig-Pohoiki-Beach-cropped.jpg)

Shown in 2019, this 8 million neuron neuromorphic system used 64 Intel Loihi chips. Source: [Tim Herman/Intel](https://newsroom.intel.la/news/la-pohoiki-beach-de-intel-un-sistema-neuromorfico-de-64-chips-presenta-resultados-de-investigacion-innovadores-english-only/)

其他团体如 IBM、T2、英特尔、T4 和大学都试图用现有的 CMOS 技术制造基于硬件的 SNN 芯片。英特尔的一个这样的平台，称为 Loihi 芯片，可以整合到一个更大的系统中。去年早些时候(2020 年)，英特尔研究[在一个网格中使用 768 个 Loihi 芯片来实现最近邻搜索](https://arxiv.org/abs/2004.12691)。1 亿个神经元的机器显示了有希望的结果，为具有大型预计算索引的系统提供了更好的延迟，并允许在 O(1)时间内插入新条目。

人类大脑项目是一个大规模的项目，致力于加深我们对生物神经网络的理解。他们有一个名为 BrainScaleS-1 waferscale 的系统，该系统依赖于神经元的模拟和混合信号模拟。20 个晶片(每个 8 英寸)组成了大脑尺度，每个晶片有 200，000 个模拟神经元。一个后续系统(BrainScaleS-2)目前正在开发中，估计完成日期为 2023 年。

蓝色大脑项目是由瑞士领导的一项研究工作，旨在模拟一个生物学上详细的老鼠大脑。虽然不是人类大脑，但他们发表的[论文和模型](https://www.epfl.ch/research/domains/bluebrain/blue-brains-scientific-milestones/)在推进我们向有用的神经形态人工神经网络发展的过程中是无价的。

大家一致认为，我们在努力创造可以做大量有意义工作的东西方面处于非常非常早期的阶段。最大的拦路虎是，我们仍然不太了解大脑是如何连接和学习的。当你开始接触这种规模的网络时，最大的挑战是如何训练它们。

## 我们需要神经形态吗？

我们甚至不需要神经形态的硬件。像[反向强化学习(](https://arxiv.org/abs/1806.06877) [IRL](https://arxiv.org/abs/1806.06877) [)](https://arxiv.org/abs/1806.06877) 这样的技术允许机器创建奖励函数，而不是网络。通过简单地观察行为，你可以模拟行为想要做什么，并通过一个习得的奖励函数来重现它，以确保专家(被观察的演员)做得最好。进一步的研究是处理次优专家来推断他们正在做什么和他们正在试图做什么。

许多人将继续推进我们已经拥有的具有更好奖励功能的简化网络。例如，IEEE 最近的一篇关于用一个简单的三层神经网络复制蜻蜓大脑的一部分的文章已经展示了用一种有条不紊的、知情的方法取得的巨大成果。虽然被训练的神经网络在野外的表现不如蜻蜓，但很难说这是因为蜻蜓与其他昆虫相比具有更优越的飞行能力。

每年我们都看到深度学习技术产生更好、更强大的结果。似乎仅仅在一两篇论文中，一个给定的领域就从有趣到惊人到令人瞠目结舌。鉴于我们没有水晶球，谁知道呢？也许如果我们继续沿着这条路走下去，我们会发现一些更具普遍性的东西，可以适应我们现有的深度学习网络。

## 现在一个黑客能做什么？

如果你想参与神经形态的研究，本文中提到的许多项目都是开源的，它们的数据集和模型可以在 GitHub 和其他 SVN 发行商那里获得。有很多令人难以置信的开源项目，比如耶鲁大学的[神经元](https://www.neuron.yale.edu/neuron/)或 [NEST SNN 模拟器](https://www.nest-simulator.org/)。许多人分享他们在[开源大脑](https://www.opensourcebrain.org/)上的实验。你甚至可以创建自己的神经形态硬件，就像 2015 年 Hackaday 奖项目“神经细胞”一样。如果你想了解更多，这份 2017 年的神经形态硬件[调查是当时该领域令人难以置信的快照。](https://arxiv.org/abs/1705.06963)

虽然还有很长的路要走，但神经形态计算的未来看起来很有希望。